##### COMMAND EXECUTION PLAN â€” INTELLIGENCE STATUS LOGGING + ERROR DETAILS

# WHY
# - Surface root causes quickly during burn-in.
# - Correlate requests, timings, and DB health to synthetic checks.
# - Keep prod safe: redact stack traces unless explicitly enabled.

# IMPACT IF IGNORED
# - 500s stay opaque. Slower triage. Higher MTTR.

# 1) PATCH: routes
applypatch << 'PATCH'
*** Begin Patch
*** Update File: modules/auto_intel/routes.py
@@
-from flask import Blueprint, jsonify, request
-import time, uuid, logging, traceback
-from .db_adapter import get_health_stats, get_recent_errors, get_forecasts_summary
-from sentry_sdk import capture_exception
+from flask import Blueprint, jsonify, request
+import os, time, uuid, logging, traceback
+from .db_adapter import get_health_stats, get_forecasts_summary
+try:
+    from sentry_sdk import capture_exception
+except Exception:  # sentry optional
+    def capture_exception(_e):  # noqa: N802
+        return None
@@
 @bp.route("/api/intelligence/status", methods=["GET"])
 def intelligence_status():
-    t0 = time.time()
-    cid = request.headers.get("X-Request-ID") or uuid.uuid4().hex
-    logger = logging.getLogger("intel")
+    t0 = time.time()
+    cid = request.headers.get("X-Request-ID") or uuid.uuid4().hex
+    logger = logging.getLogger("intel.status")
+    reveal_errors = os.getenv("INTEL_DEBUG_ERRORS", "false").lower() in ("1","true","yes","on")
     try:
-        health = get_health_stats()
-        forecasts = get_forecasts_summary(limit=5)
-        duration_ms = int((time.time()-t0)*1000)
-        payload = {
-            "ok": True,
-            "health": health,
-            "forecasts": forecasts,
-            "meta": {
-                "correlation_id": cid,
-                "duration_ms": duration_ms,
-                "ts": int(time.time()),
-                "version": "v8.0-burnin"
-            }
-        }
-        logger.info("intel_status.ok", extra={"cid": cid, "duration_ms": duration_ms, "health_err_rate": health.get("error_rate")})
-        return jsonify(payload), 200
+        health = get_health_stats()
+        forecasts = get_forecasts_summary(limit=5)
+        duration_ms = int((time.time() - t0) * 1000)
+        payload = {
+            "ok": True,
+            "health": health,
+            "forecasts": forecasts,
+            "meta": {
+                "correlation_id": cid,
+                "duration_ms": duration_ms,
+                "ts": int(time.time()),
+                "version": "v8.0-burnin",
+            },
+        }
+        logger.info(
+            "intel_status.ok",
+            extra={
+                "cid": cid,
+                "duration_ms": duration_ms,
+                "err_rate": health.get("error_rate"),
+                "db_latency_ms": health.get("db_latency_ms"),
+            },
+        )
+        return jsonify(payload), 200
     except Exception as e:
-        duration_ms = int((time.time()-t0)*1000)
-        err = {
-            "ok": False,
-            "error": {
-                "type": e.__class__.__name__,
-                "message": str(e),
-                "trace": traceback.format_exc().splitlines()[-5:]
-            },
-            "meta": {
-                "correlation_id": cid,
-                "duration_ms": duration_ms
-            }
-        }
-        logger.error("intel_status.error", extra={"cid": cid, "duration_ms": duration_ms, "error": str(e)})
-        try:
-            capture_exception(e)
-        except Exception:
-            pass
-        return jsonify(err), 500
+        duration_ms = int((time.time() - t0) * 1000)
+        trace_tail = traceback.format_exc().splitlines()[-6:]
+        logger.error(
+            "intel_status.error",
+            extra={
+                "cid": cid,
+                "duration_ms": duration_ms,
+                "etype": e.__class__.__name__,
+                "emsg": str(e),
+            },
+        )
+        capture_exception(e)
+        error_body = {
+            "ok": False,
+            "error": {
+                "type": e.__class__.__name__,
+                "message": str(e)[:500],
+            },
+            "meta": {
+                "correlation_id": cid,
+                "duration_ms": duration_ms,
+                "ts": int(time.time()),
+                "version": "v8.0-burnin",
+            },
+        }
+        if reveal_errors:
+            error_body["error"]["trace_tail"] = trace_tail
+        return jsonify(error_body), 500
*** End Patch
PATCH

# 2) OPTIONAL: structured JSON logging (skip if already configured)
if ! grep -q "JsonFormatter" config/logging.py 2>/dev/null; then
  mkdir -p config
  cat > config/logging.py << 'PY'
import json, logging, sys, time
_SKIP = {"args","msg","levelname","name","created","msecs","relativeCreated","exc_info","exc_text","stack_info"}
class JsonFormatter(logging.Formatter):
    def format(self, r):
        base = {"ts": int(time.time()), "level": r.levelname, "logger": r.name, "msg": r.getMessage()}
        for k,v in r.__dict__.items():
            if k not in _SKIP and not k.startswith("_"):
                base[k]=v
        return json.dumps(base)
h = logging.StreamHandler(sys.stdout); h.setFormatter(JsonFormatter())
root = logging.getLogger(); root.setLevel(logging.INFO); root.addHandler(h)
PY
fi

# 3) TESTS: endpoint behavior incl. error path
mkdir -p tests
cat > tests/test_intelligence_status.py << 'PY'
def test_status_ok(client, monkeypatch):
    from modules.auto_intel import routes
    monkeypatch.setenv("INTEL_DEBUG_ERRORS", "false")
    monkeypatch.setattr(routes, "get_health_stats", lambda: {"error_rate": 0.0, "db_latency_ms": 3})
    monkeypatch.setattr(routes, "get_forecasts_summary", lambda limit=5: [])
    rv = client.get("/api/intelligence/status", headers={"X-Request-ID":"abc"})
    assert rv.status_code == 200
    data = rv.get_json()
    assert data["ok"] is True
    assert "correlation_id" in data["meta"]
    assert data["meta"]["duration_ms"] >= 0

def test_status_error_details(client, monkeypatch):
    from modules.auto_intel import routes
    def boom(): raise RuntimeError("boom")
    monkeypatch.setenv("INTEL_DEBUG_ERRORS", "true")
    monkeypatch.setattr(routes, "get_health_stats", boom)
    rv = client.get("/api/intelligence/status")
    assert rv.status_code == 500
    data = rv.get_json()
    assert data["ok"] is False
    assert data["error"]["type"] == "RuntimeError"
    assert "trace_tail" in data["error"]
PY

# 4) PREVIEW: run and inspect logs
export INTEL_DEBUG_ERRORS=true
curl -i https://api.levqor.ai/api/intelligence/status || true
# If running locally:
# flask --app app.py run &
# curl -s localhost:5000/api/intelligence/status | jq .
# tail -n 50 /tmp/logs/levqor-backend_*.log | grep intel_status

# 5) ROLLBACK
# git restore --source=HEAD~1 modules/auto_intel/routes.py || git checkout -- modules/auto_intel/routes.py

# 6) CI SMOKE
pytest -q tests/test_intelligence_status.py
##### END